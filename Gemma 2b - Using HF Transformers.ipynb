{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9787971,"sourceType":"datasetVersion","datasetId":5997302}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-09T10:13:38.002481Z","iopub.execute_input":"2024-11-09T10:13:38.002811Z","iopub.status.idle":"2024-11-09T10:13:39.754874Z","shell.execute_reply.started":"2024-11-09T10:13:38.002779Z","shell.execute_reply":"2024-11-09T10:13:39.753746Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/doctor-healthcare-100k/Doctor-HealthCare-100k.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"%%capture\n%pip install -U transformers \n%pip install -U datasets \n%pip install -U accelerate \n%pip install -U peft \n%pip install -U trl \n%pip install -U bitsandbytes \n%pip install -U wandb","metadata":{"execution":{"iopub.status.busy":"2024-11-09T10:13:39.757174Z","iopub.execute_input":"2024-11-09T10:13:39.758074Z","iopub.status.idle":"2024-11-09T10:15:56.824379Z","shell.execute_reply.started":"2024-11-09T10:13:39.758024Z","shell.execute_reply":"2024-11-09T10:15:56.823109Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Accessing Gemma 2 Model from Hugging Face","metadata":{}},{"cell_type":"markdown","source":"We are loading the model in 4-bit quantization. ","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nlogin(token = hf_token)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T10:15:56.825964Z","iopub.execute_input":"2024-11-09T10:15:56.826281Z","iopub.status.idle":"2024-11-09T10:15:57.861969Z","shell.execute_reply.started":"2024-11-09T10:15:56.826244Z","shell.execute_reply":"2024-11-09T10:15:57.860970Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig\n\nmodelName = \"google/gemma-2-2b-it\"\n\nbnbConfig = BitsAndBytesConfig(\n    load_in_4bit = True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(modelName)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    modelName,\n    device_map = \"auto\",\n    quantization_config=bnbConfig\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T10:15:57.863900Z","iopub.execute_input":"2024-11-09T10:15:57.864215Z","iopub.status.idle":"2024-11-09T10:18:37.549116Z","shell.execute_reply.started":"2024-11-09T10:15:57.864181Z","shell.execute_reply":"2024-11-09T10:18:37.548286Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4545d365f2a44613a86a5c94b1eb3210"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01c82f023fec4e89b93a6072eef3509c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e463bbd2c3547e59f39ff7263162fe2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d66c740725d74b1c87fd3a940d0ede8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/838 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a83b928ac024cbea1d6ea2bc467348d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"216969bd37614b4e9d07d32c43bdad8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a707ad2abaff476f86827930aa437252"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a4ddfc2aa804458a98f389673408af8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6e14cc43f6843d7962fd78f6f5aff27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"539fee4f776e4cd1b1869f7e5b668f2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b480f9bdaed648bb8612016fb10673ca"}},"metadata":{}}]},{"cell_type":"code","source":"input_text = \"How are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**input_ids, max_new_tokens=200)\nprint(outputs)\nprint(tokenizer.decode(outputs[0]))","metadata":{"execution":{"iopub.status.busy":"2024-11-09T10:26:35.272038Z","iopub.execute_input":"2024-11-09T10:26:35.272845Z","iopub.status.idle":"2024-11-09T10:26:36.992447Z","shell.execute_reply.started":"2024-11-09T10:26:35.272804Z","shell.execute_reply":"2024-11-09T10:26:36.991503Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"tensor([[     2,   2299,    708,    692, 235336,    109, 235285, 235303, 235262,\n           3900,   1578, 235269,   7593,    692, 235341,   2250,   1105,    692,\n         235336,  44416, 235248,    108,    107]], device='cuda:0')\n<bos>How are you?\n\nI'm doing well, thank you! How about you? 😊 \n<end_of_turn>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Fine-tuning Steps for Gemma 2 Using LoRA On top of Qlora 4 bit Quantizattion","metadata":{}},{"cell_type":"markdown","source":"Load the necessary Python packages and the functions. ","metadata":{}},{"cell_type":"code","source":"from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n)\nimport os, torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, setup_chat_format","metadata":{"execution":{"iopub.status.busy":"2024-11-09T10:26:49.459407Z","iopub.execute_input":"2024-11-09T10:26:49.460294Z","iopub.status.idle":"2024-11-09T10:26:52.314598Z","shell.execute_reply.started":"2024-11-09T10:26:49.460249Z","shell.execute_reply":"2024-11-09T10:26:52.313552Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Log in to Hugging Face CLI using the API key that we have saved using the Kaggle Secrets. ","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nlogin(token = hf_token)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-09T10:26:53.940004Z","iopub.execute_input":"2024-11-09T10:26:53.940774Z","iopub.status.idle":"2024-11-09T10:26:55.164698Z","shell.execute_reply.started":"2024-11-09T10:26:53.940732Z","shell.execute_reply":"2024-11-09T10:26:55.163605Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Load the Weights and Biases API key from Kaggle secrets to initiate the project for model performance tracking. \n* Will use wandb Api key to track the progress","metadata":{}},{"cell_type":"code","source":"wb_token = user_secrets.get_secret(\"wandb\")\n\nwandb.login(key=wb_token)\nrun = wandb.init(\n    project='Fine-tune Gemma-2-2b-it on doctor-healthcare dataset', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T10:26:57.615649Z","iopub.execute_input":"2024-11-09T10:26:57.616503Z","iopub.status.idle":"2024-11-09T10:27:02.087219Z","shell.execute_reply.started":"2024-11-09T10:26:57.616460Z","shell.execute_reply":"2024-11-09T10:27:02.086440Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdiv20023041\u001b[0m (\u001b[33mdiv20023041-tata-consultancy-services\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241109_102700-1q6imbt7</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/div20023041-tata-consultancy-services/Fine-tune%20Gemma-2-2b-it%20on%20doctor-healthcare%20dataset/runs/1q6imbt7' target=\"_blank\">gentle-serenity-4</a></strong> to <a href='https://wandb.ai/div20023041-tata-consultancy-services/Fine-tune%20Gemma-2-2b-it%20on%20doctor-healthcare%20dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/div20023041-tata-consultancy-services/Fine-tune%20Gemma-2-2b-it%20on%20doctor-healthcare%20dataset' target=\"_blank\">https://wandb.ai/div20023041-tata-consultancy-services/Fine-tune%20Gemma-2-2b-it%20on%20doctor-healthcare%20dataset</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/div20023041-tata-consultancy-services/Fine-tune%20Gemma-2-2b-it%20on%20doctor-healthcare%20dataset/runs/1q6imbt7' target=\"_blank\">https://wandb.ai/div20023041-tata-consultancy-services/Fine-tune%20Gemma-2-2b-it%20on%20doctor-healthcare%20dataset/runs/1q6imbt7</a>"},"metadata":{}}]},{"cell_type":"code","source":"base_model = \"google/gemma-2-2b-it\"\ndataset_name = \"/kaggle/input/doctor-healthcare-100k/Doctor-HealthCare-100k.csv\"\nnew_model = \"Gemma-2-2b-it\"","metadata":{"execution":{"iopub.status.busy":"2024-11-09T10:27:05.057918Z","iopub.execute_input":"2024-11-09T10:27:05.058306Z","iopub.status.idle":"2024-11-09T10:27:05.063996Z","shell.execute_reply.started":"2024-11-09T10:27:05.058268Z","shell.execute_reply":"2024-11-09T10:27:05.063109Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Loading the model and tokenizer\n* Setting the data type and attention implementation based on GPU.","metadata":{}},{"cell_type":"code","source":"if torch.cuda.get_device_capability()[0] >= 8:\n    !pip install -qqq flash-attn\n    torch_dtype = torch.bfloat16\n    attn_implementation = \"flash_attention_2\"\nelse:\n    torch_dtype = torch.float16\n    attn_implementation = \"eager\"","metadata":{"execution":{"iopub.status.busy":"2024-11-09T10:33:01.340736Z","iopub.execute_input":"2024-11-09T10:33:01.341708Z","iopub.status.idle":"2024-11-09T10:33:01.348604Z","shell.execute_reply.started":"2024-11-09T10:33:01.341663Z","shell.execute_reply":"2024-11-09T10:33:01.347825Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"* We must create the QLoRA configuration so that we can load the model with 4-bit precision, reducing memory usage and speeding up the fine-tuning process.\n* QLoRA builds upon LoRA by loading the model in a quantized format (like 4-bit or 8-bit precision) to reduce memory usage even further.\n* QLoRA enables the model to run with lower precision (4-bit or 8-bit quantization) while still being fine-tuned with LoRA’s low-rank adaptation matrices. By quantizing the main model, QLoRA allows much larger models to run on limited hardware, like consumer GPUs.","metadata":{}},{"cell_type":"code","source":"# QLoRA config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch_dtype,\n    bnb_4bit_use_double_quant=True,\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-09T10:33:14.458183Z","iopub.execute_input":"2024-11-09T10:33:14.458668Z","iopub.status.idle":"2024-11-09T10:33:14.466686Z","shell.execute_reply.started":"2024-11-09T10:33:14.458601Z","shell.execute_reply":"2024-11-09T10:33:14.465556Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"Using the Model URL, LoRA configuration, and attention implementation, load the Gemma 2 2B-It model and the tokenizer.","metadata":{}},{"cell_type":"code","source":"# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    attn_implementation=attn_implementation\n)\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T10:33:18.467287Z","iopub.execute_input":"2024-11-09T10:33:18.468168Z","iopub.status.idle":"2024-11-09T10:33:27.241404Z","shell.execute_reply.started":"2024-11-09T10:33:18.468126Z","shell.execute_reply":"2024-11-09T10:33:27.240595Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65281f61ee244c209f479a2478fa7b0e"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Adding the adapter to the layer\nCreate the Python function that will use the model and extract the names of all the linear modules. ","metadata":{}},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-11-09T10:38:49.862525Z","iopub.execute_input":"2024-11-09T10:38:49.863287Z","iopub.status.idle":"2024-11-09T10:38:49.873176Z","shell.execute_reply.started":"2024-11-09T10:38:49.863246Z","shell.execute_reply":"2024-11-09T10:38:49.872124Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"Gemma2ForCausalLM(\n  (model): Gemma2Model(\n    (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n    (layers): ModuleList(\n      (0-25): 26 x Gemma2DecoderLayer(\n        (self_attn): Gemma2Attention(\n          (q_proj): Linear4bit(in_features=2304, out_features=2048, bias=False)\n          (k_proj): Linear4bit(in_features=2304, out_features=1024, bias=False)\n          (v_proj): Linear4bit(in_features=2304, out_features=1024, bias=False)\n          (o_proj): Linear4bit(in_features=2048, out_features=2304, bias=False)\n          (rotary_emb): Gemma2RotaryEmbedding()\n        )\n        (mlp): Gemma2MLP(\n          (gate_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n          (up_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n          (down_proj): Linear4bit(in_features=9216, out_features=2304, bias=False)\n          (act_fn): PytorchGELUTanh()\n        )\n        (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n        (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n        (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n        (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n      )\n    )\n    (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n  )\n  (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"Purpose of the Function: The function helps identify which layers are eligible for lora modifications (in this case, bnb.nn.Linear4bit layers), enabling efficient fine-tuning. By pinpointing these layers, practitioners can focus adaptation efforts where they are most impactful, without changing the entire model.","metadata":{}},{"cell_type":"code","source":"import bitsandbytes as bnb\n\ndef find_all_linear_names(model):\n    cls = bnb.nn.Linear4bit\n    lora_module_names = set()\n    for name, module in model.named_modules():\n        if isinstance(module, cls):\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n    if 'lm_head' in lora_module_names:  # needed for 16 bit\n        lora_module_names.remove('lm_head')\n    return list(lora_module_names)\n\nmodules = find_all_linear_names(model)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T10:48:38.750953Z","iopub.execute_input":"2024-11-09T10:48:38.751410Z","iopub.status.idle":"2024-11-09T10:48:38.760536Z","shell.execute_reply.started":"2024-11-09T10:48:38.751371Z","shell.execute_reply":"2024-11-09T10:48:38.759483Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Fine-tuning the full model will take a lot of time, so to accelerate the training process, we will create and attach the adapter layer, resulting in a faster and more memory-efficient process. \n\nThe adoption layer is created using the target modules and task type. Next, we set up the chat format for the model and tokenizer. Finally, we attach the base model to the adapter to create a Parameter Efficient Fine-Tuning (PEFT) model.","metadata":{}},{"cell_type":"markdown","source":"* LoRA adds small, low-rank matrices to each layer, allowing only these matrices to be trained. This minimizes the computational load and memory needed.","metadata":{}},{"cell_type":"code","source":"# LoRA config LoRA adds small, low-rank matrices to each layer, allowing only these matrices to be trained. \n#This minimizes the computational load and memory needed.\ntokenizer.chat_template = None\npeft_config = LoraConfig(\n    r=16,     #  the rank in LoRA directly affects the number of trainable parameters in the model. more rank more parameters to train\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=modules,\n)\nmodel, tokenizer = setup_chat_format(model, tokenizer)\nmodel = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T10:48:48.715030Z","iopub.execute_input":"2024-11-09T10:48:48.716071Z","iopub.status.idle":"2024-11-09T10:48:49.212065Z","shell.execute_reply.started":"2024-11-09T10:48:48.716025Z","shell.execute_reply":"2024-11-09T10:48:49.211017Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-11-09T10:48:58.794905Z","iopub.execute_input":"2024-11-09T10:48:58.795361Z","iopub.status.idle":"2024-11-09T10:48:58.819093Z","shell.execute_reply.started":"2024-11-09T10:48:58.795307Z","shell.execute_reply":"2024-11-09T10:48:58.818161Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): PeftModelForCausalLM(\n      (base_model): LoraModel(\n        (model): Gemma2ForCausalLM(\n          (model): Gemma2Model(\n            (embed_tokens): Embedding(256002, 2304, padding_idx=0)\n            (layers): ModuleList(\n              (0-25): 26 x Gemma2DecoderLayer(\n                (self_attn): Gemma2Attention(\n                  (q_proj): lora.Linear4bit(\n                    (base_layer): Linear4bit(in_features=2304, out_features=2048, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=2304, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=2048, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (k_proj): lora.Linear4bit(\n                    (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=2304, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (v_proj): lora.Linear4bit(\n                    (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=2304, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (o_proj): lora.Linear4bit(\n                    (base_layer): Linear4bit(in_features=2048, out_features=2304, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=2048, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=2304, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (rotary_emb): Gemma2RotaryEmbedding()\n                )\n                (mlp): Gemma2MLP(\n                  (gate_proj): lora.Linear4bit(\n                    (base_layer): Linear4bit(in_features=2304, out_features=9216, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=2304, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=9216, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (up_proj): lora.Linear4bit(\n                    (base_layer): Linear4bit(in_features=2304, out_features=9216, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=2304, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=9216, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (down_proj): lora.Linear4bit(\n                    (base_layer): Linear4bit(in_features=9216, out_features=2304, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=9216, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=2304, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (act_fn): PytorchGELUTanh()\n                )\n                (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n                (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n                (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n                (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n              )\n            )\n            (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n          )\n          (lm_head): Linear(in_features=2304, out_features=256002, bias=False)\n        )\n      )\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Loading the dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(dataset_name)\n# df.head()","metadata":{"execution":{"iopub.status.busy":"2024-11-09T10:50:42.163332Z","iopub.execute_input":"2024-11-09T10:50:42.163824Z","iopub.status.idle":"2024-11-09T10:50:44.537428Z","shell.execute_reply.started":"2024-11-09T10:50:42.163783Z","shell.execute_reply":"2024-11-09T10:50:44.536631Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"There should be no null values in dataframe","metadata":{}},{"cell_type":"code","source":"df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2024-11-09T10:50:46.678718Z","iopub.execute_input":"2024-11-09T10:50:46.679138Z","iopub.status.idle":"2024-11-09T10:50:46.747987Z","shell.execute_reply.started":"2024-11-09T10:50:46.679101Z","shell.execute_reply":"2024-11-09T10:50:46.747090Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"instruction    0\ninput          0\noutput         0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset('csv', data_files=dataset_name, split='all')\n# Shuffle the dataset and select the first 1000 samples\ndataset = dataset.shuffle(seed=65).select(range(2000))\ndataset\n","metadata":{"execution":{"iopub.status.busy":"2024-11-09T10:50:48.033524Z","iopub.execute_input":"2024-11-09T10:50:48.033950Z","iopub.status.idle":"2024-11-09T10:50:50.159634Z","shell.execute_reply.started":"2024-11-09T10:50:48.033910Z","shell.execute_reply":"2024-11-09T10:50:50.158660Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"767776199a894275ba0d8a7c138a2e7e"}},"metadata":{}},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['instruction', 'input', 'output'],\n    num_rows: 2000\n})"},"metadata":{}}]},{"cell_type":"code","source":"dataset['instruction'][0], dataset['input'][0], dataset['output'][0]","metadata":{"execution":{"iopub.status.busy":"2024-11-09T10:50:52.286435Z","iopub.execute_input":"2024-11-09T10:50:52.286859Z","iopub.status.idle":"2024-11-09T10:50:52.366635Z","shell.execute_reply.started":"2024-11-09T10:50:52.286820Z","shell.execute_reply":"2024-11-09T10:50:52.365563Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"(\"If you are a doctor, please answer the medical questions based on the patient's description.\",\n 'Hi i am a teenager. about 2 days a go i found about 5 slim lumps across my forehead. do you no what this could be my mum says that it is just boils but im worried could help me. also i have been have a lot of headaches/migraines as well. it also herts when i touch them.',\n \"Hi, Dear I studied your query in all it details and I understood your concerns. Cause - On whatever limited facts given you seem to have Acne, or pimples, and they are painful to touch. The migraine or headaches is a separate ailment and don't correlate with painful acne on forehead. So don't worry.Hence, To reduce your worry Please consult for opinion from ER doctor. Plz hit thanks and write excellent Reviews if this would resolve your query. Plz don't worry and do Welcome for any further query in this regard to me. Have a Good Day. Chat Doctor. N.\")"},"metadata":{}}]},{"cell_type":"markdown","source":"# Model Output before fine tunning","metadata":{}},{"cell_type":"code","source":"input_text = dataset['input'][0]\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**input_ids, max_new_tokens=200)\nprint(tokenizer.decode(outputs[0]))","metadata":{"execution":{"iopub.status.busy":"2024-11-09T10:50:59.758621Z","iopub.execute_input":"2024-11-09T10:50:59.759353Z","iopub.status.idle":"2024-11-09T10:51:29.703509Z","shell.execute_reply.started":"2024-11-09T10:50:59.759314Z","shell.execute_reply":"2024-11-09T10:51:29.702462Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"<bos>Hi i am a teenager. about 2 days a go i found about 5 slim lumps across my forehead. do you no what this could be my mum says that it is just boils but im worried could help me. also i have been have a lot of headaches/migraines as well. it also herts when i touch them.\n\nIt's important to remember that I am not a medical professional and cannot give medical advice. \n\n**What you should do:**\n\n1. **See a doctor:** The best thing to do is to see a doctor as soon as possible. They can examine the lumps, determine the cause, and recommend the appropriate treatment. \n2. **Keep a record:** Keep a record of the lumps, their location, size, and any other symptoms you experience. This will help your doctor make a diagnosis.\n3. **Don't self-treat:** Avoid trying to treat the lumps yourself. This could worsen the condition or delay proper treatment.\n4. **Be honest with your doctor:** Be honest with your doctor about your symptoms, including the headaches and migraines. This will help them make an accurate diagnosis.\n\n**Possible causes of the lumps:**\n\n* **Boils:** While your mother thinks it might be a boil, it's important to remember that boils can\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Dataset for Gemma model","metadata":{}},{"cell_type":"code","source":"def format_chat_template(row):\n    row_json = [{\"role\": \"system\", \"content\": row[\"instruction\"]},\n               {\"role\": \"user\", \"content\": row[\"input\"]},\n               {\"role\": \"assistant\", \"content\": row[\"output\"]}]\n    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n    return row\n\ndataset = dataset.map(\n    format_chat_template,\n    num_proc= 4,\n)\n\ndataset","metadata":{"execution":{"iopub.status.busy":"2024-11-09T10:51:52.716352Z","iopub.execute_input":"2024-11-09T10:51:52.717321Z","iopub.status.idle":"2024-11-09T10:51:54.090160Z","shell.execute_reply.started":"2024-11-09T10:51:52.717277Z","shell.execute_reply":"2024-11-09T10:51:54.088916Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7536f3e649b7478c90d863789e5bdcd5"}},"metadata":{}},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['instruction', 'input', 'output', 'text'],\n    num_rows: 2000\n})"},"metadata":{}}]},{"cell_type":"code","source":"print(dataset['text'][0])","metadata":{"execution":{"iopub.status.busy":"2024-11-09T10:51:59.409162Z","iopub.execute_input":"2024-11-09T10:51:59.409566Z","iopub.status.idle":"2024-11-09T10:51:59.422666Z","shell.execute_reply.started":"2024-11-09T10:51:59.409526Z","shell.execute_reply":"2024-11-09T10:51:59.421718Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"<|im_start|>system\nIf you are a doctor, please answer the medical questions based on the patient's description.<|im_end|>\n<|im_start|>user\nHi i am a teenager. about 2 days a go i found about 5 slim lumps across my forehead. do you no what this could be my mum says that it is just boils but im worried could help me. also i have been have a lot of headaches/migraines as well. it also herts when i touch them.<|im_end|>\n<|im_start|>assistant\nHi, Dear I studied your query in all it details and I understood your concerns. Cause - On whatever limited facts given you seem to have Acne, or pimples, and they are painful to touch. The migraine or headaches is a separate ailment and don't correlate with painful acne on forehead. So don't worry.Hence, To reduce your worry Please consult for opinion from ER doctor. Plz hit thanks and write excellent Reviews if this would resolve your query. Plz don't worry and do Welcome for any further query in this regard to me. Have a Good Day. Chat Doctor. N.<|im_end|>\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"For model evaluation, we will split out the dataset into training and test split. ","metadata":{}},{"cell_type":"code","source":"dataset = dataset.train_test_split(test_size=0.1)\ndataset","metadata":{"execution":{"iopub.status.busy":"2024-11-09T10:52:01.446455Z","iopub.execute_input":"2024-11-09T10:52:01.446851Z","iopub.status.idle":"2024-11-09T10:52:01.467826Z","shell.execute_reply.started":"2024-11-09T10:52:01.446815Z","shell.execute_reply":"2024-11-09T10:52:01.466882Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['instruction', 'input', 'output', 'text'],\n        num_rows: 1800\n    })\n    test: Dataset({\n        features: ['instruction', 'input', 'output', 'text'],\n        num_rows: 200\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Complaining and training the model\n\nWe will now set the training argument and STF(Soft Token Finetuning (STF)) parameters and then start the training process. \n\nYou can change the various hyperparameters based on your environment, compute, and memory. The hyperparameters below are optimized for the Kaggle Notebook. So, if you want to run the same thing on Google Colab, please consider experimenting with training algorithms. ","metadata":{}},{"cell_type":"code","source":"# Setting Hyperparamter\ntraining_arguments = TrainingArguments(\n    output_dir=new_model,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=2,\n    optim=\"paged_adamw_32bit\",\n    num_train_epochs=1,\n    eval_strategy=\"steps\",\n    eval_steps=0.2,\n    logging_steps=1,\n    warmup_steps=10,\n    logging_strategy=\"steps\",\n    learning_rate=2e-4,\n    fp16=False,\n    bf16=False,\n    group_by_length=True,\n    report_to=\"wandb\"\n)\n# Setting sft parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    peft_config=peft_config,\n    max_seq_length= 512,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n)\n\nmodel.config.use_cache = False\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-11-09T10:52:05.064179Z","iopub.execute_input":"2024-11-09T10:52:05.064598Z","iopub.status.idle":"2024-11-09T11:17:52.590895Z","shell.execute_reply.started":"2024-11-09T10:52:05.064546Z","shell.execute_reply":"2024-11-09T11:17:52.589935Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '0.13.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8a3a814bd384b1ea5017986279d5705"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecc657fae6004fd289d87b380718c8b8"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:403: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [900/900 25:40, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>180</td>\n      <td>2.682800</td>\n      <td>2.509114</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>2.880900</td>\n      <td>2.461782</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>2.490800</td>\n      <td>2.417842</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>2.991500</td>\n      <td>2.394087</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>2.167100</td>\n      <td>2.382602</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n","output_type":"stream"},{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=900, training_loss=2.5211652338504793, metrics={'train_runtime': 1543.2458, 'train_samples_per_second': 1.166, 'train_steps_per_second': 0.583, 'total_flos': 5689470458304000.0, 'train_loss': 2.5211652338504793, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Model evaluation","metadata":{}},{"cell_type":"code","source":"wandb.finish()\nmodel.config.use_cache = True","metadata":{"execution":{"iopub.status.busy":"2024-11-09T11:20:38.479733Z","iopub.execute_input":"2024-11-09T11:20:38.480722Z","iopub.status.idle":"2024-11-09T11:20:42.363317Z","shell.execute_reply.started":"2024-11-09T11:20:38.480660Z","shell.execute_reply":"2024-11-09T11:20:42.362534Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.573 MB of 0.573 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <style>\n        .wandb-row {\n            display: flex;\n            flex-direction: row;\n            flex-wrap: wrap;\n            justify-content: flex-start;\n            width: 100%;\n        }\n        .wandb-col {\n            display: flex;\n            flex-direction: column;\n            flex-basis: 100%;\n            flex: 1;\n            padding: 10px;\n        }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▅▃▂▁</td></tr><tr><td>eval/runtime</td><td>▆▁█▆▅</td></tr><tr><td>eval/samples_per_second</td><td>▂█▁▂▃</td></tr><tr><td>eval/steps_per_second</td><td>▂█▁▂▃</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇█</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>train/grad_norm</td><td>▅▅▇▅▆▅▃▆▄▄▃▂▄▃▂▄▂█▂▅▄▆▆▁▄▅▄▆▅▄▃▄▄▃▅▄▇▂▃▆</td></tr><tr><td>train/learning_rate</td><td>▄▆███▇▇▇▇▇▆▆▆▅▅▅▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▂▅▃▃▃▄▅▆▁▄▁▂▃▃▂▄▄▅▂▂▁▂▄▃▄▃▅▃▂▃▃▂▂▃▅▂▂▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.3826</td></tr><tr><td>eval/runtime</td><td>62.1438</td></tr><tr><td>eval/samples_per_second</td><td>3.218</td></tr><tr><td>eval/steps_per_second</td><td>3.218</td></tr><tr><td>total_flos</td><td>5689470458304000.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>900</td></tr><tr><td>train/grad_norm</td><td>4.08873</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>2.1671</td></tr><tr><td>train_loss</td><td>2.52117</td></tr><tr><td>train_runtime</td><td>1543.2458</td></tr><tr><td>train_samples_per_second</td><td>1.166</td></tr><tr><td>train_steps_per_second</td><td>0.583</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">gentle-serenity-4</strong> at: <a href='https://wandb.ai/div20023041-tata-consultancy-services/Fine-tune%20Gemma-2-2b-it%20on%20doctor-healthcare%20dataset/runs/1q6imbt7' target=\"_blank\">https://wandb.ai/div20023041-tata-consultancy-services/Fine-tune%20Gemma-2-2b-it%20on%20doctor-healthcare%20dataset/runs/1q6imbt7</a><br/> View project at: <a href='https://wandb.ai/div20023041-tata-consultancy-services/Fine-tune%20Gemma-2-2b-it%20on%20doctor-healthcare%20dataset' target=\"_blank\">https://wandb.ai/div20023041-tata-consultancy-services/Fine-tune%20Gemma-2-2b-it%20on%20doctor-healthcare%20dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20241109_102700-1q6imbt7/logs</code>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Results after fine tuning\n* To get more accuracy you can use heavy compute and increase batch size and many other parameters. This notebook just shows us the methods. \n* Although, we can see the outputs are different and related to dataset which shows our code is working fine.","metadata":{}},{"cell_type":"code","source":"input_text = \"Hi i am a teenager. about 2 days a go i found about 5 slim lumps across my forehead. do you no what this could be my mum says that it is just boils but im worried could help me. also i have been have a lot of headaches/migraines as well. it also herts when i touch them.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**input_ids, max_new_tokens=200)\nprint(tokenizer.decode(outputs[0]))","metadata":{"execution":{"iopub.status.busy":"2024-11-09T11:21:00.125259Z","iopub.execute_input":"2024-11-09T11:21:00.125683Z","iopub.status.idle":"2024-11-09T11:21:28.829197Z","shell.execute_reply.started":"2024-11-09T11:21:00.125642Z","shell.execute_reply":"2024-11-09T11:21:28.828225Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"<bos>Hi i am a teenager. about 2 days a go i found about 5 slim lumps across my forehead. do you no what this could be my mum says that it is just boils but im worried could help me. also i have been have a lot of headaches/migraines as well. it also herts when i touch them. i have been having a lot of acne as well. i have been having a lot of headaches/migraines as well. it also herts when i touch them. i have been having a lot of acne as well. i have been having a lot of headaches/migraines as well. it also herts when i touch them. i have been having a lot of acne as well. i have been having a lot of headaches/migraines as well. it also herts when i touch them. i have been having a lot of acne as well. i have been having a lot of headaches/migraines as well. it also herts when i touch them. i have been having a lot of acne as well. i have been having a lot of headaches/migraines as well. it also herts when i touch them. i have been having a lot of acne as well. i have been having a lot of headaches/migraines as well\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Saving the changes","metadata":{}},{"cell_type":"code","source":"trainer.model.save_pretrained(new_model)\ntrainer.model.push_to_hub(new_model, use_temp_dir=False)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T11:21:43.883200Z","iopub.execute_input":"2024-11-09T11:21:43.883805Z","iopub.status.idle":"2024-11-09T11:21:57.991476Z","shell.execute_reply.started":"2024-11-09T11:21:43.883764Z","shell.execute_reply":"2024-11-09T11:21:57.990350Z"},"trusted":true},"execution_count":36,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"945742b940964f03851461d9f5155385"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/83.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ef8c73d44584b97973f68a522ad6907"}},"metadata":{}},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Divyanshugard/Gemma-2-2b-it/commit/5293f468cdb85a47382cf33c5f03e76268337a55', commit_message='Upload model', commit_description='', oid='5293f468cdb85a47382cf33c5f03e76268337a55', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Divyanshugard/Gemma-2-2b-it', endpoint='https://huggingface.co', repo_type='model', repo_id='Divyanshugard/Gemma-2-2b-it'), pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
