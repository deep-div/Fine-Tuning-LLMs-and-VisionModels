{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":210412477,"sourceType":"kernelVersion"},{"sourceId":120005,"sourceType":"modelInstanceVersion","modelInstanceId":100936,"modelId":121027}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Phase 1\n* https://www.kaggle.com/code/divyanshu2000/phase-1-fine-tune-lama-3-2-qlora-lora","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-30T06:30:20.044509Z","iopub.execute_input":"2024-11-30T06:30:20.045227Z","iopub.status.idle":"2024-11-30T06:30:20.461617Z","shell.execute_reply.started":"2024-11-30T06:30:20.045188Z","shell.execute_reply":"2024-11-30T06:30:20.460701Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/phase1-fine-tune-lama-3-2-qlora-lora/__results__.html\n/kaggle/input/phase1-fine-tune-lama-3-2-qlora-lora/updated_dataset.csv\n/kaggle/input/phase1-fine-tune-lama-3-2-qlora-lora/__notebook__.ipynb\n/kaggle/input/phase1-fine-tune-lama-3-2-qlora-lora/__output__.json\n/kaggle/input/phase1-fine-tune-lama-3-2-qlora-lora/custom.css\n/kaggle/input/phase1-fine-tune-lama-3-2-qlora-lora/llama-3.2-3b-it-CustomerSupport-ChatBot/adapter_model.safetensors\n/kaggle/input/phase1-fine-tune-lama-3-2-qlora-lora/llama-3.2-3b-it-CustomerSupport-ChatBot/adapter_config.json\n/kaggle/input/phase1-fine-tune-lama-3-2-qlora-lora/llama-3.2-3b-it-CustomerSupport-ChatBot/README.md\n/kaggle/input/phase1-fine-tune-lama-3-2-qlora-lora/llama-3.2-3b-it-CustomerSupport-ChatBot/checkpoint-450/adapter_model.safetensors\n/kaggle/input/phase1-fine-tune-lama-3-2-qlora-lora/llama-3.2-3b-it-CustomerSupport-ChatBot/checkpoint-450/trainer_state.json\n/kaggle/input/phase1-fine-tune-lama-3-2-qlora-lora/llama-3.2-3b-it-CustomerSupport-ChatBot/checkpoint-450/training_args.bin\n/kaggle/input/phase1-fine-tune-lama-3-2-qlora-lora/llama-3.2-3b-it-CustomerSupport-ChatBot/checkpoint-450/adapter_config.json\n/kaggle/input/phase1-fine-tune-lama-3-2-qlora-lora/llama-3.2-3b-it-CustomerSupport-ChatBot/checkpoint-450/README.md\n/kaggle/input/phase1-fine-tune-lama-3-2-qlora-lora/llama-3.2-3b-it-CustomerSupport-ChatBot/checkpoint-450/tokenizer.json\n/kaggle/input/phase1-fine-tune-lama-3-2-qlora-lora/llama-3.2-3b-it-CustomerSupport-ChatBot/checkpoint-450/tokenizer_config.json\n/kaggle/input/phase1-fine-tune-lama-3-2-qlora-lora/llama-3.2-3b-it-CustomerSupport-ChatBot/checkpoint-450/scheduler.pt\n/kaggle/input/phase1-fine-tune-lama-3-2-qlora-lora/llama-3.2-3b-it-CustomerSupport-ChatBot/checkpoint-450/special_tokens_map.json\n/kaggle/input/phase1-fine-tune-lama-3-2-qlora-lora/llama-3.2-3b-it-CustomerSupport-ChatBot/checkpoint-450/optimizer.pt\n/kaggle/input/phase1-fine-tune-lama-3-2-qlora-lora/llama-3.2-3b-it-CustomerSupport-ChatBot/checkpoint-450/rng_state.pth\n/kaggle/input/phase1-fine-tune-lama-3-2-qlora-lora/wandb/run-20241130_061013-0pd5ngri/run-0pd5ngri.wandb\n/kaggle/input/phase1-fine-tune-lama-3-2-qlora-lora/wandb/run-20241130_061013-0pd5ngri/logs/debug.log\n/kaggle/input/phase1-fine-tune-lama-3-2-qlora-lora/wandb/run-20241130_061013-0pd5ngri/logs/debug-internal.log\n/kaggle/input/phase1-fine-tune-lama-3-2-qlora-lora/wandb/run-20241130_061013-0pd5ngri/files/output.log\n/kaggle/input/phase1-fine-tune-lama-3-2-qlora-lora/wandb/run-20241130_061013-0pd5ngri/files/requirements.txt\n/kaggle/input/phase1-fine-tune-lama-3-2-qlora-lora/wandb/run-20241130_061013-0pd5ngri/files/wandb-metadata.json\n/kaggle/input/llama-3.2/transformers/3b-instruct/1/model.safetensors.index.json\n/kaggle/input/llama-3.2/transformers/3b-instruct/1/config.json\n/kaggle/input/llama-3.2/transformers/3b-instruct/1/model-00001-of-00002.safetensors\n/kaggle/input/llama-3.2/transformers/3b-instruct/1/model-00002-of-00002.safetensors\n/kaggle/input/llama-3.2/transformers/3b-instruct/1/README.md\n/kaggle/input/llama-3.2/transformers/3b-instruct/1/USE_POLICY.md\n/kaggle/input/llama-3.2/transformers/3b-instruct/1/tokenizer.json\n/kaggle/input/llama-3.2/transformers/3b-instruct/1/tokenizer_config.json\n/kaggle/input/llama-3.2/transformers/3b-instruct/1/LICENSE.txt\n/kaggle/input/llama-3.2/transformers/3b-instruct/1/special_tokens_map.json\n/kaggle/input/llama-3.2/transformers/3b-instruct/1/.gitattributes\n/kaggle/input/llama-3.2/transformers/3b-instruct/1/generation_config.json\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Add input, your Phase 1 code outputs and llama 3.2 model from kaggle","metadata":{}},{"cell_type":"code","source":"%%capture\n%pip install -U bitsandbytes\n%pip install transformers==4.44.2\n%pip install -U accelerate\n%pip install -U peft\n%pip install -U trl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T06:30:50.988089Z","iopub.execute_input":"2024-11-30T06:30:50.988524Z","iopub.status.idle":"2024-11-30T06:31:56.565980Z","shell.execute_reply.started":"2024-11-30T06:30:50.988492Z","shell.execute_reply":"2024-11-30T06:31:56.564744Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nlogin(token = hf_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T06:33:10.812425Z","iopub.execute_input":"2024-11-30T06:33:10.813207Z","iopub.status.idle":"2024-11-30T06:33:11.400116Z","shell.execute_reply.started":"2024-11-30T06:33:10.813168Z","shell.execute_reply":"2024-11-30T06:33:11.399258Z"}},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Merging Tuned Model and Exporting Fine-tuned Llama 3.2 ","metadata":{}},{"cell_type":"code","source":"# Model\nbase_model_url = \"/kaggle/input/llama-3.2/transformers/3b-instruct/1\"\nnew_model_url  = \"/kaggle/input/phase1-fine-tune-lama-3-2-qlora-lora/llama-3.2-3b-it-CustomerSupport-ChatBot/\" #mention path as i did ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T06:35:34.637200Z","iopub.execute_input":"2024-11-30T06:35:34.637947Z","iopub.status.idle":"2024-11-30T06:35:34.641806Z","shell.execute_reply.started":"2024-11-30T06:35:34.637908Z","shell.execute_reply":"2024-11-30T06:35:34.640893Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\nfrom peft import PeftModel\nimport torch\nfrom trl import setup_chat_format\n# Reload tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(base_model_url)\n\nbase_model_reload= AutoModelForCausalLM.from_pretrained(\n    base_model_url,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T06:35:37.067013Z","iopub.execute_input":"2024-11-30T06:35:37.067310Z","iopub.status.idle":"2024-11-30T06:36:25.941224Z","shell.execute_reply.started":"2024-11-30T06:35:37.067284Z","shell.execute_reply":"2024-11-30T06:36:25.940334Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6de1a5bf4934b35bc72df2538876ad5"}},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"# Merge adapter with base model","metadata":{}},{"cell_type":"code","source":"tokenizer.chat_template = None\nbase_model_reload, tokenizer = setup_chat_format(base_model_reload, tokenizer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T06:36:31.779539Z","iopub.execute_input":"2024-11-30T06:36:31.780360Z","iopub.status.idle":"2024-11-30T06:36:38.884511Z","shell.execute_reply.started":"2024-11-30T06:36:31.780326Z","shell.execute_reply":"2024-11-30T06:36:38.883220Z"}},"outputs":[{"name":"stderr","text":"The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"model = PeftModel.from_pretrained(base_model_reload, new_model_url)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T06:36:40.527398Z","iopub.execute_input":"2024-11-30T06:36:40.528208Z","iopub.status.idle":"2024-11-30T06:36:51.539134Z","shell.execute_reply.started":"2024-11-30T06:36:40.528175Z","shell.execute_reply":"2024-11-30T06:36:51.538119Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"model = model.merge_and_unload()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T06:36:53.581095Z","iopub.execute_input":"2024-11-30T06:36:53.581781Z","iopub.status.idle":"2024-11-30T06:36:53.712545Z","shell.execute_reply.started":"2024-11-30T06:36:53.581743Z","shell.execute_reply":"2024-11-30T06:36:53.711655Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T06:36:55.825674Z","iopub.execute_input":"2024-11-30T06:36:55.826531Z","iopub.status.idle":"2024-11-30T06:36:55.834071Z","shell.execute_reply.started":"2024-11-30T06:36:55.826496Z","shell.execute_reply":"2024-11-30T06:36:55.833209Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128258, 3072)\n    (layers): ModuleList(\n      (0-27): 28 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=3072, out_features=128258, bias=False)\n)"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"instruction = \"\"\"You are a top-rated customer service agent named John. \n    Be polite to customers and answer all their questions.\n    \"\"\"\n\nmessages = [{\"role\": \"system\", \"content\": instruction},\n    {\"role\": \"user\", \"content\": \"where do i enter a different shipping address?\"}]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    \ninputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens=150, num_return_sequences=1)\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(text.split(\"assistant\")[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T06:39:41.020792Z","iopub.execute_input":"2024-11-30T06:39:41.021174Z","iopub.status.idle":"2024-11-30T06:39:48.086691Z","shell.execute_reply.started":"2024-11-30T06:39:41.021144Z","shell.execute_reply":"2024-11-30T06:39:48.085772Z"}},"outputs":[{"name":"stdout","text":"\nThank you for reaching out! I'm here to assist you with updating your shipping address. To make the necessary changes, please follow these steps:\n\n1. Log in to your account on our website.\n2. Navigate to the \"My Account\" or \"Profile\" section.\n3. Look for the \"Shipping Addresses\" or a similar option.\n4. You will find an option to \"Add a New Address\" or \"Edit Shipping Addresses\".\n5. Choose the option and enter the details of your new shipping address.\n6. Save the changes.\n\nIf you encounter any difficulties or need further assistance, please don't hesitate to let me know. I'm here to ensure that you have a seamless experience with us.system\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"Save the tokenizer and model locally. ","metadata":{}},{"cell_type":"code","source":"new_model = \"llama-3.2-3b-it-Ecommerce-ChatBot\"\n\nmodel.save_pretrained(new_model)\ntokenizer.save_pretrained(new_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T06:40:05.893211Z","iopub.execute_input":"2024-11-30T06:40:05.893971Z","iopub.status.idle":"2024-11-30T06:40:24.554004Z","shell.execute_reply.started":"2024-11-30T06:40:05.893934Z","shell.execute_reply":"2024-11-30T06:40:24.553079Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"('llama-3.2-3b-it-Ecommerce-ChatBot/tokenizer_config.json',\n 'llama-3.2-3b-it-Ecommerce-ChatBot/special_tokens_map.json',\n 'llama-3.2-3b-it-Ecommerce-ChatBot/tokenizer.json')"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"Push the tokenizer and merged model to the Hugging Face model repository.","metadata":{}},{"cell_type":"code","source":"model.push_to_hub(new_model, use_temp_dir=False)\ntokenizer.push_to_hub(new_model, use_temp_dir=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
